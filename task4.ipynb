{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "Use test-queries.tsv and candidate-passages-top1000.tsv, and implement the query likelihood language model with:\n",
    "- Laplace smoothing\n",
    "- Lidstone correction (epsilon = 0.1)\n",
    "- Dirichlet smoothing (mu = 50)\n",
    "\n",
    "Note: report scores as ln(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(text):\n",
    "    '''\n",
    "    Function that normalises text and returns tokens.\n",
    "    Input: text --> text string we want to tokenise\n",
    "    Output: tokens --> list of tokens taken from the text string\n",
    "    '''\n",
    "\n",
    "    text = text.lower() # convert all to lower case\n",
    "    text = contractions.fix(text) # expand contractions\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    tokens = text.split()\n",
    "    #Â tokens = re.findall(r'(\\b[a-z|1-9|\\S]+\\b)', text) # tokenisation\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words] # remove stop words\n",
    "    filtered_tokens = list(map(lemmatizer.lemmatize, filtered_tokens)) # lemmatization of nouns\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = pd.read_csv('candidate-passages-top1000.tsv', delimiter='\\t', header=None, names=['qid','pid','query','passage'])\n",
    "tq = pd.read_csv('test-queries.tsv', delimiter='\\t', header=None, names=['qid','query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = cp[['pid', 'passage']].copy()\n",
    "passages = passages.drop_duplicates()\n",
    "passages = passages.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index.pkl', 'rb') as f:\n",
    "    inv_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our vocabulary\n",
    "V = len(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of passages\n",
    "len_passages = {}\n",
    "\n",
    "def len_passages_func(row):\n",
    "    pid = row['pid']\n",
    "    passage = row['passage']\n",
    "    check = normalise(passage)\n",
    "    len_passages[pid] = len(check)\n",
    "\n",
    "_ = passages.apply(lambda row: len_passages_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, and keeping things separate, we compute the frequencies of the query terms separately\n",
    "qf_dict = {i:{} for i in tq['qid']}\n",
    "\n",
    "def qf_func(row):\n",
    "    qid = row['qid']\n",
    "    query = row['query']\n",
    "    check = normalise(query)\n",
    "    unique_words = list(set(check))\n",
    "\n",
    "    for item in unique_words:\n",
    "        if item in inv_index.keys():\n",
    "            qf_dict[qid][item] = check.count(item) # non-normalised frequency\n",
    "\n",
    "_ = tq.apply(lambda row: qf_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace Smoothing\n",
    "laplace_scores = np.array([[0,0,0]])\n",
    "\n",
    "for k in range(len(tq)):\n",
    "    scores = []\n",
    "    qid = tq['qid'][k]\n",
    "    query = qf_dict[qid]\n",
    "    query_words = list(query.keys())\n",
    "\n",
    "    for pid in cp.loc[cp['qid'] == tq['qid'][k]]['pid']:\n",
    "\n",
    "        dl = len_passages[pid]\n",
    "\n",
    "        lap_est = np.zeros(len(query_words))\n",
    "\n",
    "        for i in range(len(query_words)):\n",
    "            if pid in inv_index[query_words[i]]:\n",
    "                m = inv_index[query_words[i]][pid]\n",
    "            else:\n",
    "                m = 0\n",
    "            \n",
    "            lap_est[i] = (m + 1) / (dl + V)\n",
    "\n",
    "        score = np.prod(lap_est)\n",
    "        scores.append([qid,pid,np.log(score)])\n",
    "    \n",
    "    scores = np.array(scores, dtype=\"O\")\n",
    "    scores = scores[np.argsort(-scores[:,-1])] # sort in descending order\n",
    "    \n",
    "    laplace_scores = np.append(laplace_scores, scores[:100,:], axis=0)\n",
    "\n",
    "laplace_scores = laplace_scores[1:,:] # remove the [0,0,0] row we used to initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(laplace_scores).to_csv(\"laplace.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lidstone correction\n",
    "eps = 0.1\n",
    "lidstone_scores = np.array([[0,0,0]])\n",
    "\n",
    "for k in range(len(tq)):\n",
    "    scores = []\n",
    "    qid = tq['qid'][k]\n",
    "    query = qf_dict[qid]\n",
    "    query_words = list(query.keys())\n",
    "\n",
    "    for pid in cp.loc[cp['qid'] == tq['qid'][k]]['pid']:\n",
    "\n",
    "        dl = len_passages[pid]\n",
    "\n",
    "        lid_est = np.zeros(len(query_words))\n",
    "\n",
    "        for i in range(len(query_words)):\n",
    "            if pid in inv_index[query_words[i]]:\n",
    "                m = inv_index[query_words[i]][pid]\n",
    "            else:\n",
    "                m = 0\n",
    "            \n",
    "            lid_est[i] = (m + eps) / (dl + eps*V)\n",
    "\n",
    "        score = np.prod(lid_est)\n",
    "        scores.append([qid,pid,np.log(score)])\n",
    "    \n",
    "    scores = np.array(scores, dtype=\"O\")\n",
    "    scores = scores[np.argsort(-scores[:,-1])] # sort in descending order\n",
    "    \n",
    "    lidstone_scores = np.append(lidstone_scores, scores[:100,:], axis=0)\n",
    "\n",
    "lidstone_scores = lidstone_scores[1:,:] # remove the [0,0,0] row we used to initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lidstone_scores).to_csv(\"lidstone.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, let's get the frequency of each term in the inverted index in the whole collection of documents\n",
    "freq_coll = {}\n",
    "\n",
    "for key in inv_index.keys():\n",
    "    freq_coll[key] = sum(inv_index[key].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet smoothing\n",
    "mu = 50\n",
    "dirichlet_scores = np.array([[0,0,0]])\n",
    "cl = sum(freq_coll.values())\n",
    "\n",
    "for k in range(len(tq)):\n",
    "    scores = []\n",
    "    qid = tq['qid'][k]\n",
    "    query = qf_dict[qid]\n",
    "    query_words = list(query.keys())\n",
    "\n",
    "    for pid in cp.loc[cp['qid'] == tq['qid'][k]]['pid']:\n",
    "\n",
    "        dl = len_passages[pid]\n",
    "\n",
    "        dir_est = np.zeros(len(query_words))\n",
    "\n",
    "        for i in range(len(query_words)):\n",
    "            if pid in inv_index[query_words[i]]:\n",
    "                fd = inv_index[query_words[i]][pid]\n",
    "            else:\n",
    "                fd = 0\n",
    "            \n",
    "            fc = freq_coll[query_words[i]]\n",
    "\n",
    "            dir_est[i] = (dl/(dl+mu) * fd/dl) + (mu/(dl+mu) * fc/cl)\n",
    "\n",
    "        score = np.prod(dir_est)\n",
    "        scores.append([qid,pid,np.log(score)])\n",
    "    \n",
    "    scores = np.array(scores, dtype=\"O\")\n",
    "    scores = scores[np.argsort(-scores[:,-1])] # sort in descending order\n",
    "    \n",
    "    dirichlet_scores = np.append(dirichlet_scores, scores[:100,:], axis=0)\n",
    "\n",
    "dirichlet_scores = dirichlet_scores[1:,:] # remove the [0,0,0] row we used to initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dirichlet_scores).to_csv(\"dirichlet.csv\", header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "989a8ee0261a415be34d4cf0f45e98134ff6fbcaa2e29b3efcaef888d322ba01"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
