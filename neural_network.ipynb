{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4 - Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv('train_data.tsv', delimiter='\\t')\n",
    "val_dataframe = pd.read_csv('validation_data.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_train = np.loadtxt('processed_X_train_data.txt')\n",
    "y_data_train = np.loadtxt('processed_y_train_data.txt')\n",
    "X_data_val = np.loadtxt('processed_X_val_data.txt')\n",
    "y_data_val = np.loadtxt('processed_y_val_data.txt')\n",
    "\n",
    "X_data_train = torch.from_numpy(X_data_train)\n",
    "y_data_train = torch.from_numpy(y_data_train)\n",
    "X_data_val = torch.from_numpy(X_data_val)\n",
    "y_data_val = torch.from_numpy(y_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Weights\n",
    "\n",
    "# train_weights = torch.zeros(len(y_data_train))\n",
    "# train_weights[y_data_train == 1] = 1 - len(y_data_train[y_data_train == 1]) / len(y_data_train)\n",
    "# train_weights[y_data_train == 0] = 1 - len(y_data_train[y_data_train == 0]) / len(y_data_train)\n",
    "\n",
    "# val_weights = torch.ones(len(y_data_val))\n",
    "# val_weights[y_data_val == 1] = 1 - len(y_data_val[y_data_val == 1]) / len(y_data_val)\n",
    "# val_weights[y_data_val == 0] = 1 - len(y_data_val[y_data_val == 0]) / len(y_data_val)\n",
    "\n",
    "train_weights = torch.zeros(len(y_data_train))\n",
    "train_weights[y_data_train == 1] = 0.9\n",
    "train_weights[y_data_train == 0] = 0.1\n",
    "\n",
    "val_weights = torch.ones(len(y_data_val))\n",
    "val_weights[y_data_val == 1] = 0.9\n",
    "val_weights[y_data_val == 0] = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fch = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x): # x is the input layer\n",
    "        x = self.dropout(self.relu(self.fc1(x))) # hidden layer 1\n",
    "        x = self.dropout(self.relu(self.fch(x))) # hidden layer 2\n",
    "        x = self.dropout(self.relu(self.fch(x))) # hidden layer 3\n",
    "        x = self.dropout(self.relu(self.fch(x))) # hidden layer 4\n",
    "        x = self.dropout(self.relu(self.fch(x))) # hidden layer 5\n",
    "        x = self.dropout(self.relu(self.fch(x))) # hidden layer 6\n",
    "        output = self.sigmoid(self.fc2(x)) # output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "model = Feedforward(input_dim, hidden_dim)\n",
    "criterion = torch.nn.BCELoss(weight=val_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.06480520963668823\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_data_val.float())\n",
    "before_train = criterion(y_pred.squeeze(), y_data_val.float())\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.06556297838687897\n",
      "Epoch 2: train loss: 0.05949615687131882\n",
      "Epoch 3: train loss: 0.052813392132520676\n",
      "Epoch 4: train loss: 0.04570887237787247\n",
      "Epoch 5: train loss: 0.03852064162492752\n",
      "Epoch 6: train loss: 0.03178482875227928\n",
      "Epoch 7: train loss: 0.025889089331030846\n",
      "Epoch 8: train loss: 0.020978713408112526\n",
      "Epoch 9: train loss: 0.017357463017106056\n",
      "Epoch 10: train loss: 0.015001445077359676\n",
      "Epoch 11: train loss: 0.013711116276681423\n",
      "Epoch 12: train loss: 0.0134737528860569\n",
      "Epoch 13: train loss: 0.013710679486393929\n",
      "Epoch 14: train loss: 0.014265657402575016\n",
      "Epoch 15: train loss: 0.014615845866501331\n",
      "Epoch 16: train loss: 0.014834432862699032\n",
      "Epoch 17: train loss: 0.015052419155836105\n",
      "Epoch 18: train loss: 0.014501589350402355\n",
      "Epoch 19: train loss: 0.014044645242393017\n",
      "Epoch 20: train loss: 0.013431651517748833\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "criterion = torch.nn.BCELoss(weight=train_weights)\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_data_train.float())\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_data_train.float())\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch+1, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.009435766376554966\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "criterion = torch.nn.BCELoss(weight=val_weights)\n",
    "y_pred = model(X_data_val.float())\n",
    "after_train = criterion(y_pred.squeeze(), y_data_val.float()) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = y_pred.detach().numpy()\n",
    "y_pred_val = y_pred_val.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export NN.txt ranking\n",
    "\n",
    "unique_qids = val_dataframe['qid'].copy()\n",
    "unique_qids = unique_qids.drop_duplicates()\n",
    "unique_qids = unique_qids.reset_index(drop=True)\n",
    "unique_qids = np.array(unique_qids) # vector of unique qid in the validation set\n",
    "\n",
    "qid_pid_val = val_dataframe[['qid','pid']].copy()\n",
    "qid_pid_val = np.array(qid_pid_val) # (n,2) array, where each row corresponds to the (qid, pid) pair of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_scores = pd.DataFrame(columns=['qid', 'A', 'pid', 'rank', 'score', 'algoname']) # dataframe where we store global results for all queries\n",
    "\n",
    "for qid in unique_qids:\n",
    "    output_info = pd.DataFrame(columns=['qid', 'A', 'pid', 'rank', 'score', 'algoname']) # dataframe where we store \n",
    "                                                                                         # the results for the current query\n",
    "    qid_pairs = qid_pid_val[qid_pid_val[:,0] == qid]\n",
    "    qid_rels = y_pred_val[qid_pid_val[:,0] == qid]\n",
    "    indxs = np.argsort(qid_rels)[::-1]\n",
    "    sorted_qid_pairs = qid_pairs[indxs]\n",
    "    sorted_qid_rels = qid_rels[indxs]\n",
    "    \n",
    "    # Now we just get the top 100 scores (if they are available)\n",
    "    top_sorted_qid_pairs = sorted_qid_pairs[:100,:]\n",
    "    top_sorted_qid_rels = sorted_qid_rels[:100]\n",
    "\n",
    "    # Prepare the array with 'A2'\n",
    "    A2 = np.array(['A2'] * len(top_sorted_qid_rels))\n",
    "\n",
    "    # Prepare the array with the ranks\n",
    "    rank = np.array(range(1,len(top_sorted_qid_rels)+1))\n",
    "\n",
    "    # Prepare the array with the algonamme 'LR'\n",
    "    algoname = np.array(['NN'] * len(top_sorted_qid_rels))\n",
    "\n",
    "    # Put everything together in the output_info dataframe\n",
    "    output_info['qid'] = top_sorted_qid_pairs[:,0]\n",
    "    output_info['A'] = A2\n",
    "    output_info['pid'] = top_sorted_qid_pairs[:,1]\n",
    "    output_info['rank'] = rank\n",
    "    output_info['score'] = top_sorted_qid_rels\n",
    "    output_info['algoname'] = algoname\n",
    "\n",
    "    # Append this query dataframe to the one with global results\n",
    "    NN_scores = NN_scores.append(output_info, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ranking file as NN.txt\n",
    "np.savetxt(r'NN.txt', NN_scores.values, fmt=['%d','%s','%d','%d','%f','%s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess performance of model on validation data\n",
    "For this part we use the mAP and NDCG functions defiend in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataframes for the functions\n",
    "tq = val_dataframe[['qid', 'queries']].copy()\n",
    "tq = tq.drop_duplicates()\n",
    "tq = tq.reset_index(drop=True)\n",
    "\n",
    "relevancies = val_dataframe[['qid', 'pid', 'relevancy']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ranking data created in previous part\n",
    "NN_ranking = pd.read_csv('NN.txt', delimiter=' ', header=None, names=['qid', 'A', 'pid', 'rank', 'score', 'algoname'])\n",
    "ranking = NN_ranking[['qid', 'pid', 'score']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the AP and NDCG functions from Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAP function\n",
    "def AP(queries, ranking, relevancies, k):\n",
    "    '''\n",
    "    Function that computes the Average Precision (AP) metric for each query in 'queries', based on a ranking determined by \n",
    "    a retrieval model, where queries are matched with passages from most relevant to least relevant, and based on relevancies \n",
    "    between queries and passages.\n",
    "\n",
    "    Inputs:\n",
    "    queries = data frame of queries for which you want to calculate the AP metric (contains qid and actual query)\n",
    "    ranking = data frame of queries and passages pairs, where higher score pairs are ranked higher (for each query)\n",
    "    relevancies = data frame of relevancies between each possible (qid,pid) pair\n",
    "    k = top k passages you want to take into account when calculating the AP metric\n",
    "\n",
    "    Outputs:\n",
    "    APs = list of AP@k metric for each query, in the same order of appearance as the input list 'queries'\n",
    "    mAP = mean Average Precision of all the queries\n",
    "    '''\n",
    "\n",
    "    APs = []\n",
    "\n",
    "    for q in queries['qid']:\n",
    "        AP_values = []\n",
    "        cum_rel = 0 # cumulative number of relevant passages found in the ranking\n",
    "\n",
    "        max_k = len(ranking[ranking['qid'] == q])\n",
    "        iter = min(k,max_k) # This is because we some queries do not have that many candidate passages\n",
    "\n",
    "        for i in range(1,iter+1):\n",
    "            p = int(ranking[ranking['qid'] == q].reset_index(drop=True).iloc[i-1]['pid'])\n",
    "            relevancy = relevancies[(relevancies['qid'] == q) & (relevancies['pid'] == p)]['relevancy'].values.item()\n",
    "            if relevancy != 0: # we operate when we encounter a relevant passage\n",
    "                cum_rel += relevancy\n",
    "                AP_values.append(cum_rel / i) \n",
    "\n",
    "        if len(AP_values) != 0:    \n",
    "            APs.append(sum(AP_values)/len(AP_values))\n",
    "        else: # we do this to avoid the computing error of dividing 0/0\n",
    "            APs.append(0)\n",
    "\n",
    "    mAP = np.mean(APs)\n",
    "\n",
    "    return APs, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG function\n",
    "def NDCG(queries, ranking, relevancies, k):\n",
    "    '''\n",
    "    Function that computes the Normalized Discounted Cumulative Gain (NDCG) metric for each query in 'queries', \n",
    "    based on a ranking determined by a retrieval model, where queries are matched with passages from most relevant \n",
    "    to least relevant, and based on relevancies between queries and passages.\n",
    "\n",
    "    Inputs:\n",
    "    queries = data frame of queries for which you want to calculate the AP metric (contains qid and actual query)\n",
    "    ranking = data frame of queries and passages pairs, where higher score pairs are ranked higher (for each query)\n",
    "    relevancies = data frame of relevancies between each possible (qid,pid) pair\n",
    "    k = top k passages you want to take into account when calculating the AP metric\n",
    "\n",
    "    Outputs:\n",
    "    NDCGs = list of AP@k metric for each query, in the same order of appearance as the input list 'queries'\n",
    "    mNDCG = mean Average Precision of all the queries\n",
    "    '''\n",
    "\n",
    "    NDCGs = []\n",
    "\n",
    "    for q in queries['qid']:\n",
    "        DCG = 0\n",
    "        IDCG = 0\n",
    "\n",
    "        max_k = len(ranking[ranking['qid'] == q])\n",
    "        iter = min(k,max_k) # This is because we some queries do not have that many candidate passages\n",
    "\n",
    "        # Get the relevancies for the ideal ranking (of the top k candidates??? - doesn't matter for our data tho), in order\n",
    "        sorted_revs = relevancies[relevancies['qid'] == q].sort_values(by=['relevancy'], ascending=False)['relevancy'].values\n",
    "\n",
    "        for i in range(1,iter+1):\n",
    "            IDCG += (2**sorted_revs[i-1] - 1)/np.log2(i+1)\n",
    "\n",
    "            p = int(ranking[ranking['qid'] == q].reset_index(drop=True).iloc[i-1]['pid'])\n",
    "            rel = relevancies[(relevancies['qid'] == q) & (relevancies['pid'] == p)]['relevancy'].values.item()\n",
    "            DCG += (2**rel - 1)/np.log2(i+1)\n",
    "\n",
    "        if IDCG != 0:\n",
    "            NDCGs.append(DCG/IDCG)\n",
    "        else: # we do this to avoid the computing error of dividing 0/0\n",
    "            NDCGs.append(0)\n",
    "\n",
    "    mNDCG = np.mean(NDCGs)\n",
    "\n",
    "    return NDCGs, mNDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_3, mAP_3 = AP(tq, ranking, relevancies, 3)\n",
    "AP_10, mAP_10 = AP(tq, ranking, relevancies, 10)\n",
    "AP_100, mAP_100 = AP(tq, ranking, relevancies, 100)\n",
    "\n",
    "NDCG_3, mNDCG_3 = NDCG(tq, ranking, relevancies, 3)\n",
    "NDCG_10, mNDCG_10 = NDCG(tq, ranking, relevancies, 10)\n",
    "NDCG_100, mNDCG_100 = NDCG(tq, ranking, relevancies, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007113821138211382 0.009674865881311875 0.012530086828311094\n",
      "0.008409972794300774 0.013653655839167635 0.03325898104597199\n"
     ]
    }
   ],
   "source": [
    "print(mAP_3, mAP_10, mAP_100)\n",
    "\n",
    "print(mNDCG_3, mNDCG_10, mNDCG_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Task 4"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "989a8ee0261a415be34d4cf0f45e98134ff6fbcaa2e29b3efcaef888d322ba01"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
