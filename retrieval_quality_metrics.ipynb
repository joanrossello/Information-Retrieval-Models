{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Evaluating Retrieval Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement BM25 code from Coursework 1\n",
    "NOTE: none of this code is new. It is all reused from the previous coursework, and adapted to the validation dataset.\n",
    "\n",
    "Here we implement the BM25 code from Coursework 1, with the same text normalisation function, and all, but we do it on the validation data instesd of the candidate passages top1000 data.\n",
    "From this we will obtain a file where for each query we have the top 100 (max) passages ranked from highest BM25 score to lowest. \n",
    "Then we use the ranking and the relevance scores from the validation set to calculate the performance metrics for the BM25 retrieval model.\n",
    "\n",
    "This is the nature of the other tasks - build a retrieval model (here we already have it from the previous coursework, but in the other tasks we need to make it and train it with the training set), and then once we have the retrieval model, we feed the validation data to it, obtain the sorted (qid,pid) pairs in ranking order with their respective scores (whatever score each model uses), and then we use that ranking alongside the relevance scores in the validation set to calculate the performance metrics of the retrieval model:  mAP and NDCG scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(text):\n",
    "    '''\n",
    "    Function that normalises text and returns tokens.\n",
    "    Input: text --> text string we want to tokenise\n",
    "    Output: tokens --> list of tokens taken from the text string\n",
    "    '''\n",
    "\n",
    "    text = text.lower() # convert all to lower case\n",
    "    text = contractions.fix(text) # expand contractions\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) # remove punctuation\n",
    "    tokens = text.split() # tokenisation\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words] # remove stop words\n",
    "    filtered_tokens = list(map(lemmatizer.lemmatize, filtered_tokens)) # lemmatization of nouns\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = pd.read_csv('validation_data.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancies = cp[['qid', 'pid', 'relevancy']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = cp[['pid', 'passage']].copy()\n",
    "passages = passages.drop_duplicates()\n",
    "passages = passages.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tq = cp[['qid', 'queries']].copy()\n",
    "tq = tq.drop_duplicates()\n",
    "tq = tq.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = {}\n",
    "\n",
    "def inverted_index_func(row):\n",
    "    pid = row['pid']\n",
    "    passage = row['passage']\n",
    "    check = normalise(passage)\n",
    "    unique_words = list(set(check))\n",
    "\n",
    "    for item in unique_words:\n",
    "        if item not in inv_index:\n",
    "            inv_index[item] = {}\n",
    "        if item in inv_index:\n",
    "            inv_index[item][pid] = check.count(item) # Frequency of the word (not normalised)\n",
    "\n",
    "_ = passages.apply(lambda row: inverted_index_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFÂ·IDF passages\n",
    "p_tfidf = {i:{} for i in passages['pid']}\n",
    "\n",
    "# Also calculate the length of passages which will be useful in BM25\n",
    "len_passages = {}\n",
    "N = len(passages)\n",
    "\n",
    "def p_tfidf_func(row):\n",
    "    pid = row['pid']\n",
    "    passage = row['passage']\n",
    "    check = normalise(passage)\n",
    "    len_passages[pid] = len(check)\n",
    "    unique_words = list(set(check))\n",
    "\n",
    "    for item in unique_words:\n",
    "        tf = inv_index[item][pid] / len(check) # Normalise the frequency from inverted index\n",
    "        idf = np.log(N/len(inv_index[item]))\n",
    "        p_tfidf[pid][item] = tf*idf\n",
    "    p_tfidf[pid]['norm_of_passage'] = np.linalg.norm(np.array(list(map(p_tfidf[pid].get, p_tfidf[pid].keys())))) # add norm of the passage\n",
    "\n",
    "_ = passages.apply(lambda row: p_tfidf_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, and keeping things separate, we compute the frequencies of the query terms separately\n",
    "qf_dict = {i:{} for i in tq['qid']}\n",
    "\n",
    "def qf_func(row):\n",
    "    qid = row['qid']\n",
    "    query = row['queries']\n",
    "    check = normalise(query)\n",
    "    unique_words = list(set(check))\n",
    "\n",
    "    for item in unique_words:\n",
    "        if item in inv_index.keys():\n",
    "            qf_dict[qid][item] = check.count(item) # non-normalised frequency\n",
    "\n",
    "_ = tq.apply(lambda row: qf_func(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(n, N, k1, k2, b, dl, avdl, f, qf, r=0, R=0):\n",
    "    '''\n",
    "    BM25 score calculating function\n",
    "\n",
    "    Inputs\n",
    "    n: (vector of integers) number of total docs. containing each term in the query (each vector element corresponds to a term)\n",
    "    N: (integer) total number of documents we have\n",
    "    k1: (scalar) constant parameter set empirically\n",
    "    k2: (scalar) constant parameter set empirically\n",
    "    b: (scalar) constant parameter set empirically\n",
    "    dl: (integer) document length --> number of tokens\n",
    "    avdl: (scalar) average document length --> average number of tokens in the set of documents\n",
    "    f: (vector of integers) frequency in the document of each term in the query (each vector element corresponds to a term)\n",
    "    qf: (vector of integers) frequency in the document of each term in the query (each vector element corresponds to a term)\n",
    "    r: (vector of integers) number of relevant docs. containing each term in the query (each vector element corresponds to a term)\n",
    "    R: (integer) total number of relevant documents\n",
    "\n",
    "    Note that if we do not have information about relevance feedback, r and R are set to 0\n",
    "\n",
    "    Output\n",
    "    score: (scalar) BM25 score of a document with respect to a query\n",
    "\n",
    "    '''\n",
    "\n",
    "    K = k1 * ((1 - b) + b * (dl/avdl))\n",
    "\n",
    "    score = np.sum( np.log( ((r+0.5)/(R-r+0.5)) / ((n-r+0.5)/(N-n-R+r*0.5)) ) * (((k1+1)*f)/(K+f)) * (((k2+1)*qf)/(k2+qf)) )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(passages)\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "b = 0.75\n",
    "avdl = sum(len_passages.values()) / len(len_passages)\n",
    "\n",
    "BM25_scores = np.array([[0,0,0]])\n",
    "\n",
    "for k in range(len(tq)):\n",
    "    scores = []\n",
    "    qid = tq['qid'][k]\n",
    "    query = qf_dict[qid]\n",
    "    query_words = query.keys()\n",
    "\n",
    "    for pid in cp.loc[cp['qid'] == tq['qid'][k]]['pid']:\n",
    "        \n",
    "        passage = p_tfidf[pid]\n",
    "        passage_words = passage.keys()\n",
    "\n",
    "        common = list(set(query_words) & set(passage_words))\n",
    "\n",
    "        dl = len_passages[pid]\n",
    "\n",
    "        n, f, qf = np.zeros(len(common)), np.zeros(len(common)), np.zeros(len(common))\n",
    "        for i in range(len(common)):\n",
    "            n[i] = len(inv_index[common[i]])\n",
    "            f[i] = inv_index[common[i]][pid]\n",
    "            qf[i] = qf_dict[qid][common[i]]\n",
    "\n",
    "        score = BM25(n,N,k1,k2,b,dl,avdl,f,qf)\n",
    "\n",
    "        scores.append([qid,pid,score])\n",
    "    \n",
    "    scores = np.array(scores, dtype=\"O\")\n",
    "    scores = scores[np.argsort(-scores[:,-1])] # sort in descending order\n",
    "    \n",
    "    BM25_scores = np.append(BM25_scores, scores[:100,:], axis=0)\n",
    "\n",
    "BM25_scores = BM25_scores[1:,:] # remove the [0,0,0] row we used to initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(BM25_scores).to_csv(\"bm25_cw2.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to calculate mAP and NDCG\n",
    "Note that our rankings are in a .csv file where we have (qid, pid, score), and the (qid, pid) pairs are in order from best to worse, for each qid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the rankings as a data frame\n",
    "ranking = pd.read_csv('bm25_cw2.csv', delimiter=',', header=None, names=['qid','pid','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAP function\n",
    "def AP(queries, ranking, relevancies, k):\n",
    "    '''\n",
    "    Function that computes the Average Precision (AP) metric for each query in 'queries', based on a ranking determined by \n",
    "    a retrieval model, where queries are matched with passages from most relevant to least relevant, and based on relevancies \n",
    "    between queries and passages.\n",
    "\n",
    "    Inputs:\n",
    "    queries = data frame of queries for which you want to calculate the AP metric (contains qid and actual query)\n",
    "    ranking = data frame of queries and passages pairs, where higher score pairs are ranked higher (for each query)\n",
    "    relevancies = data frame of relevancies between each possible (qid,pid) pair\n",
    "    k = top k passages you want to take into account when calculating the AP metric\n",
    "\n",
    "    Outputs:\n",
    "    APs = list of AP@k metric for each query, in the same order of appearance as the input list 'queries'\n",
    "    mAP = mean Average Precision of all the queries\n",
    "    '''\n",
    "\n",
    "    APs = []\n",
    "\n",
    "    for q in queries['qid']:\n",
    "        AP_values = []\n",
    "        cum_rel = 0 # cumulative number of relevant passages found in the ranking\n",
    "\n",
    "        max_k = len(ranking[ranking['qid'] == q])\n",
    "        iter = min(k,max_k) # This is because we some queries do not have that many candidate passages\n",
    "\n",
    "        for i in range(1,iter+1):\n",
    "            p = int(ranking[ranking['qid'] == q].reset_index(drop=True).iloc[i-1]['pid'])\n",
    "            relevancy = relevancies[(relevancies['qid'] == q) & (relevancies['pid'] == p)]['relevancy'].values.item()\n",
    "            if relevancy != 0: # we operate when we encounter a relevant passage\n",
    "                cum_rel += relevancy\n",
    "                AP_values.append(cum_rel / i) \n",
    "\n",
    "        if len(AP_values) != 0:    \n",
    "            APs.append(sum(AP_values)/len(AP_values))\n",
    "        else: # we do this to avoid the computing error of dividing 0/0\n",
    "            APs.append(0)\n",
    "\n",
    "    mAP = np.mean(APs)\n",
    "\n",
    "    return APs, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_3, mAP_3 = AP(tq, ranking, relevancies, 3)\n",
    "AP_10, mAP_10 = AP(tq, ranking, relevancies, 10)\n",
    "AP_100, mAP_100 = AP(tq, ranking, relevancies, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1923635307781649 0.22908750898733476 0.24050810167578454\n"
     ]
    }
   ],
   "source": [
    "print(mAP_3, mAP_10, mAP_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG function\n",
    "def NDCG(queries, ranking, relevancies, k):\n",
    "    '''\n",
    "    Function that computes the Normalized Discounted Cumulative Gain (NDCG) metric for each query in 'queries', \n",
    "    based on a ranking determined by a retrieval model, where queries are matched with passages from most relevant \n",
    "    to least relevant, and based on relevancies between queries and passages.\n",
    "\n",
    "    Inputs:\n",
    "    queries = data frame of queries for which you want to calculate the AP metric (contains qid and actual query)\n",
    "    ranking = data frame of queries and passages pairs, where higher score pairs are ranked higher (for each query)\n",
    "    relevancies = data frame of relevancies between each possible (qid,pid) pair\n",
    "    k = top k passages you want to take into account when calculating the AP metric\n",
    "\n",
    "    Outputs:\n",
    "    NDCGs = list of AP@k metric for each query, in the same order of appearance as the input list 'queries'\n",
    "    mNDCG = mean Average Precision of all the queries\n",
    "    '''\n",
    "\n",
    "    NDCGs = []\n",
    "\n",
    "    for q in queries['qid']:\n",
    "        DCG = 0\n",
    "        IDCG = 0\n",
    "\n",
    "        max_k = len(ranking[ranking['qid'] == q])\n",
    "        iter = min(k,max_k) # This is because we some queries do not have that many candidate passages\n",
    "\n",
    "        # Get the relevancies for the ideal ranking in order\n",
    "        sorted_revs = relevancies[relevancies['qid'] == q].sort_values(by=['relevancy'], ascending=False)['relevancy'].values\n",
    "\n",
    "        for i in range(1,iter+1):\n",
    "            IDCG += (2**sorted_revs[i-1] - 1)/np.log2(i+1)\n",
    "\n",
    "            p = int(ranking[ranking['qid'] == q].reset_index(drop=True).iloc[i-1]['pid'])\n",
    "            rel = relevancies[(relevancies['qid'] == q) & (relevancies['pid'] == p)]['relevancy'].values.item()\n",
    "            DCG += (2**rel - 1)/np.log2(i+1)\n",
    "\n",
    "        if IDCG != 0:\n",
    "            NDCGs.append(DCG/IDCG)\n",
    "        else: # we do this to avoid the computing error of dividing 0/0\n",
    "            NDCGs.append(0)\n",
    "\n",
    "    mNDCG = np.mean(NDCGs)\n",
    "\n",
    "    return NDCGs, mNDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDCG_3, mNDCG_3 = NDCG(tq, ranking, relevancies, 3)\n",
    "NDCG_10, mNDCG_10 = NDCG(tq, ranking, relevancies, 10)\n",
    "NDCG_100, mNDCG_100 = NDCG(tq, ranking, relevancies, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2111449285596187 0.2870114730902176 0.3571446023375422\n"
     ]
    }
   ],
   "source": [
    "print(mNDCG_3, mNDCG_10, mNDCG_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Task 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "989a8ee0261a415be34d4cf0f45e98134ff6fbcaa2e29b3efcaef888d322ba01"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
